{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Landcover dataset extraction\n",
    "\n",
    "Author: Martina Kauzlaric (martina.kauzlaric@unibe.ch)\n",
    "\n",
    "This notebook is used to retrieve and concatenate the landcover dataset into a table for publication alongisde the used data.\n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python=3.13.2\n",
    "* Jupyter\n",
    "* os\n",
    "* numpy=2.2.4\n",
    "* xarray=2024.11.0\n",
    "* pandas=2.2.3\n",
    "* geopandas=1.0.1\n",
    "* tqdm=4.67.1\n",
    "\n",
    "Check the Github repository for an environment_lancover.yml and environment_camels_chem_landcover_dwnlCLMSdata.yml (here for downloading additionally the data via CLMS API) for conda environments file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* ?\n",
    "\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "\n",
    "## References\n",
    "* https://land.copernicus.eu/en/products/corine-land-cover\n",
    "## Observations\n",
    "* Part of the data is interpolated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all variables\n",
    "%reset -f\n",
    "#Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import box\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a0050",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Set (relative) path to your local directory\n",
    "# PATH = \"..\"\n",
    "PATH = \"S:\\\\CAMELS-CH\\\\CAMELS-chem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set directories\n",
    "GIS_dir = os.path.join(PATH,\"data\\\\GIS\")\n",
    "# Define shapefile with the catchments\n",
    "catchments_shp = os.path.join(GIS_dir,\"shapefile_catchments\\\\camels_ch_chem_catchment_boundaries.shp\")\n",
    "#Add subfolder to GIS_dir for CORINE Landcover data\n",
    "GIS_dir = os.path.join(GIS_dir, \"CORINE_Landcover\")  \n",
    "PATH_OUTPUT = os.path.join(PATH,\"results\\\\catchment_aggregated_data\\\\landcover\")\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "# Note: the directories are created in the order they are listed here, so if you want to change the structure, do it here.\n",
    "if not os.path.isdir(GIS_dir):\n",
    "    os.makedirs(GIS_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.isdir(os.path.join(PATH, \"results\")):\n",
    "    os.makedirs(os.path.join(PATH, \"results\"), exist_ok=True)\n",
    "\n",
    "if not os.path.isdir(os.path.join(PATH, \"results\\\\catchment_aggregated_data\")):\n",
    "    os.makedirs(os.path.join(PATH, \"results\\\\catchment_aggregated_data\"), exist_ok=True)\n",
    "\n",
    "if not os.path.isdir(PATH_OUTPUT):\n",
    "    os.makedirs(PATH_OUTPUT, exist_ok=True)\n",
    "\n",
    "##Change to directory to where you want to store the results    \n",
    "os.chdir(PATH_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7675d",
   "metadata": {},
   "source": [
    "# Download CORINE Landcover data\n",
    "* Here following the code to autmomatically download the Landcover data\n",
    "* Note: you nned to be regostered on CLMS and create an API token, plese refer to https://land.copernicus.eu/en/how-to-guides/how-to-download-spatial-data/how-to-create-api-tokens\n",
    "* after you created you private key please follow the steps here below\n",
    "* The user and alternatively also download manually the data under https://land.copernicus.eu/en/products/corine-land-cover\n",
    "=> then skip this part and gp to import data!\n",
    "\n",
    "## Requirements additionally to those listed above\n",
    "**Python:**\n",
    "\n",
    "* pyjwt= 2.10.1\n",
    "* cryptography = 44.0.1\n",
    "* fiona = 1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional libraries to be uploaded if not already installed\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import jwt  # PyJWT\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "import zipfile\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your token using the private key JSON file\n",
    "# Load credentials from file\n",
    "with open(r\"S:\\CAMELS-CH\\CAMELS-chem\\privatekey_API_CMLS.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "# Prepare the JWT (JSON Web Token)\n",
    "now = int(time.time())\n",
    "payload = {\n",
    "    \"iss\": creds[\"client_id\"],\n",
    "    \"sub\": creds[\"user_id\"],\n",
    "    \"aud\": creds[\"token_uri\"],\n",
    "    \"iat\": now,\n",
    "    \"exp\": now + 3600,  # expires in 1 hour\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98140d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_key = serialization.load_pem_private_key(\n",
    "    creds[\"private_key\"].encode(), password=None\n",
    ")\n",
    "\n",
    "jwt_token = jwt.encode(payload, private_key, algorithm=\"RS256\")\n",
    "\n",
    "# Request the access token\n",
    "response = requests.post(\n",
    "    creds[\"token_uri\"],\n",
    "    data={\"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\", \"assertion\": jwt_token},\n",
    ")\n",
    "\n",
    "if response.ok:\n",
    "    token = response.json()[\"access_token\"]\n",
    "    print(\"✅ Access token successfully retrieved!\")\n",
    "else:\n",
    "    raise Exception(f\"❌ Failed to get access token: {response.status_code} {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8aaa61",
   "metadata": {},
   "source": [
    "Be careful:\n",
    "https://eea.github.io/clms-api-docs/download.html#download-prepackaged-files\n",
    "\n",
    "BoundingBox”: [max.lat,max.lon,min.lat,min.lon] which is the same as [N,E,S,W]\n",
    "Note: Longitude is typically represented by the X-coordinate, and Latitude is represented by the Y-coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and reproject catchment shapefile\n",
    "catchments = gpd.read_file(catchments_shp)\n",
    "#catchments_3035 = catchments.to_crs(\"EPSG:3035\")\n",
    "# Convert geometry to GeoJSON format (MultiPolygon)\n",
    "#bounds = catchments_3035.total_bounds  # [minx, miny, maxx, maxy]\n",
    "#geometry_json = catchments_3035.geometry.union_all().__geo_interface__\n",
    "# Transform the geometry back to WGS84 (EPSG:4326) \n",
    "# => this is the projection needed to download the data by a bounding box!!\n",
    "catchments_wgs84 = catchments.to_crs(\"EPSG:4326\")\n",
    "bounds = catchments_wgs84.total_bounds\n",
    "# Define the buffer amount in decimal degrees (we add some buffer to the bounds)\n",
    "# Note: the buffer is in decimal degrees, so 0.1 =~ 10 km\n",
    "buffer = 0.1\n",
    "\n",
    "# Add buffer to the bounds\n",
    "buffered_bounds = [\n",
    "    bounds[0] - buffer,  # minx - buffer\n",
    "    bounds[1] - buffer,  # miny - buffer\n",
    "    bounds[2] + buffer,  # maxx + buffer\n",
    "    bounds[3] + buffer,  # maxy + buffer\n",
    "]\n",
    "\n",
    "# Convert NumPy float64 to standard Python float\n",
    "buffered_bounds = [float(x) for x in buffered_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bounds:\", bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3678f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Buffered bounds:\", buffered_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setup the base request\n",
    "url = \"https://land.copernicus.eu/api/@datarequest_post\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bbe41",
   "metadata": {},
   "source": [
    "You find the **UID** (here *DataseID*) and the **@id** (here *DownloadID*) for the different datasets on https://eea.github.io/clms-api-docs/download.html#download-prepackaged-files\n",
    "under **Find the items to be downloaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Make list with dataset info by year (UID = Dataset ID, ID = Vector GDB ID)\n",
    "datasets = {\n",
    "    \"2000\": {\n",
    "        \"DatasetID\": \"6704f90ca82e4f228a46111519f8978e\",\n",
    "        \"DownloadID\": \"1009310e-2cd8-481c-b15b-aee3f0406098\"\n",
    "    },\n",
    "    \"2006\": {\n",
    "        \"DatasetID\": \"d443c86fec2f49e08ff12c7decdbf2af\",\n",
    "        \"DownloadID\": \"3936f6a5-9157-4e76-9fc7-4e14668c81ef\"\n",
    "    },\n",
    "    \"2012\": {\n",
    "        \"DatasetID\": \"a5ee71470be04d66bcff498f94ceb5dc\",\n",
    "        \"DownloadID\": \"cff14ee5-bafb-46f4-a1b2-2cd6f4049514\"\n",
    "    },\n",
    "    \"2018\": {\n",
    "        \"DatasetID\": \"0407d497d3c44bcd93ce8fd5bf78596a\",\n",
    "        \"DownloadID\": \"1bda2fbd-3230-42ba-98cf-69c96ac063bc\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b915742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. #Define polling function to check if the task is completed and\n",
    "# loop over each year\n",
    "# # Note:this first part is not so important/optional, but it is good to keep track of the progress of the download\n",
    "#   the requests will be sent anyway (even if the download takes longer than 10 minutes)\n",
    "def poll_task_status(task_id, headers, max_wait=600, interval=10):\n",
    "    \"\"\"Poll Copernicus API until the task is ready or timeout.\"\"\"\n",
    "    task_url = f\"https://land.copernicus.eu/api/@tasks/{task_id}\"\n",
    "    waited = 0\n",
    "\n",
    "    while waited < max_wait:\n",
    "        response = requests.get(task_url, headers=headers)\n",
    "        if response.ok:\n",
    "            status = response.json().get(\"Status\", \"\").lower()\n",
    "            if status == \"completed\":\n",
    "                download_url = response.json().get(\"DownloadUrl\")\n",
    "                return download_url\n",
    "            elif status == \"failed\":\n",
    "                raise RuntimeError(f\"Task {task_id} failed.\")\n",
    "        time.sleep(interval)\n",
    "        waited += interval\n",
    "\n",
    "    raise TimeoutError(f\"Task {task_id} did not complete in time.\") \n",
    "\n",
    "for year, ids in datasets.items():\n",
    "    payload = {\n",
    "        \"Datasets\": [ {\n",
    "            \"DatasetID\": ids[\"DatasetID\"],\n",
    "            \"DatasetDownloadInformationID\": ids[\"DownloadID\"],\n",
    "            \"OutputFormat\": \"GDB\",\n",
    "            \"OutputGCS\": \"EPSG:3035\",\n",
    "            \"BoundingBox\": list(buffered_bounds)  # \"BoundingBox\":  must be in WGS84!!\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        task_id = response.json()[\"TaskIds\"][0][\"TaskID\"]\n",
    "        print(f\"[{year}] Submitted task {task_id}, polling for result...\")\n",
    "\n",
    "        try:\n",
    "            download_url = poll_task_status(task_id, headers)\n",
    "            if download_url:\n",
    "                print(f\"[{year}] Ready: {download_url}\")\n",
    "                response_file = requests.get(download_url)\n",
    "                out_path = os.path.join(PATH_OUTPUT, f\"CLC_{year}.zip\")\n",
    "                with open(out_path, \"wb\") as f:\n",
    "                    f.write(response_file.content)\n",
    "                print(f\"[{year}] Downloaded to: {out_path}\")\n",
    "            else:\n",
    "                print(f\"[{year}] No download URL found after polling.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{year}] Error: {e}\")\n",
    "    else:\n",
    "        print(f\"[{year}] Failed request: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1a4eb",
   "metadata": {},
   "source": [
    "**Note: data might be queued,depending on the load in the CLMS download process**\n",
    "\n",
    " *this can last 5-10min for a region like Switzerland (hydrological Switzerland is about 58'000 km^2)\n",
    " Note: If it last longer it means you might have to adapt the length of expiry duration defined above (exp) or split the download process and regenerate the access token!*\n",
    "\n",
    " **Once you receive the email confirming the download is ready, download the data and save them in the GIS_dir directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0659c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Unzip the downloaded files\n",
    "import zipfile\n",
    "\n",
    "# Folder to extract contents\n",
    "extract_dir = os.path.join(GIS_dir, \"CLC_downloads\")\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Loop through and unzip\n",
    "for file in os.listdir(GIS_dir):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(GIS_dir, file)\n",
    "        extract_path = os.path.join(extract_dir, file.replace(\".zip\", \"\"))\n",
    "        print(\"Zip path:\", zip_path)\n",
    "        print(\"Extr path:\", extract_path)\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"✅ Unzipped: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List zipped files in GIS_dir\n",
    "zipdirs = os.listdir(GIS_dir)\n",
    "zipfiles = [file for file in zipdirs if file.endswith(\".zip\")]\n",
    "print(\"Zip files:\", zipfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6be49",
   "metadata": {},
   "source": [
    "Let us free some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ce549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete each zip file\n",
    "for zipfile in zipfiles:\n",
    "    zip_path = os.path.join(GIS_dir, zipfile)\n",
    "    os.remove(zip_path)\n",
    "    print(f\"✅ Deleted: {zipfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bebe4ab",
   "metadata": {},
   "source": [
    "The downloaded data are .gdb, so we need to extract the data (*layer*) to a shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through unzipped folders\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.endswith(\".gdb\"):\n",
    "            gdb_path = os.path.join(root, dir_name)\n",
    "            print(f\"📂 GDB directory found: {gdb_path}\")\n",
    "            # List layers\n",
    "            layers = fiona.listlayers(gdb_path)\n",
    "            print(f\"  📄 Available layers: {layers}\")\n",
    "\n",
    "            # Export each layer to shapefile\n",
    "            for layer in layers:\n",
    "                gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "                out_shp = os.path.join(GIS_dir, f\"{layer}.shp\")\n",
    "                gdf.to_file(out_shp)\n",
    "                print(f\"✅ Exported: {out_shp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c718a9",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54907f",
   "metadata": {},
   "source": [
    "# Import data\n",
    "* Load catchments and look at full table\n",
    "\n",
    "*Note: Run the next two lines only if you downloaded the CORINE data manually (and didn't load yet the catchments shape file)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de985775",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments = gpd.read_file(catchments_shp)\n",
    "catchments_3035 = catchments.to_crs(\"EPSG:3035\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments[\"bafu_id\"] = catchments[\"gauge_id\"]\n",
    "catchments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16526672",
   "metadata": {},
   "source": [
    "Now we extract landcover attributes as area percentages as we did for CAMELS-CH (see also https://github.com/camels-ch/camels-ch/blob/main/landcover_attributes/corine_landcover_CH.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacfd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dictionary of CORINE data shapefiles in GIS_dir\n",
    "import re\n",
    "\n",
    "clc_by_year = {}\n",
    "\n",
    "# Loop through files and extract year\n",
    "for filename in os.listdir(GIS_dir):\n",
    "    if filename.endswith(\".shp\"):\n",
    "        # Try to find the year using regex\n",
    "        match = re.search(r'CLC(20\\d\\d)', filename)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            # Remove the .shp extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            clc_by_year[year] = name\n",
    "\n",
    "print(\"📁 Detected CORINE files by year:\")\n",
    "print(clc_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56c52d",
   "metadata": {},
   "source": [
    "Now we define helper functions we nned to extract and reclassify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def reclass_clc(clc, code_column=None):\n",
    "    \"\"\"Reclassify CORINE land cover codes into CAMELS-CH land use categories.\"\"\"\n",
    "    reclass_dict = {\n",
    "        111: \"urban_perc\", 112: \"urban_perc\", 121: \"urban_perc\", 122: \"urban_perc\", 123: \"urban_perc\", 124: \"urban_perc\",\n",
    "        131: \"loose_rock_perc\", 132: \"loose_rock_perc\", 133: \"loose_rock_perc\",\n",
    "        141: \"grass_perc\", 142: \"urban_perc\",\n",
    "        211: \"crop_perc\", 212: \"crop_perc\", 213: \"crop_perc\",\n",
    "        221: \"scrub_perc\", 222: \"scrub_perc\", 223: \"scrub_perc\",\n",
    "        231: \"grass_perc\",\n",
    "        241: \"crop_perc\", 242: \"crop_perc\", 243: \"crop_perc\", 244: \"scrub_perc\",\n",
    "        311: \"dwood_perc\", 312: \"ewood_perc\", 313: \"mixed_wood_perc\",\n",
    "        321: \"grass_perc\", 322: \"wetlands_perc\", 323: \"scrub_perc\", 324: \"scrub_perc\",\n",
    "        331: \"loose_rock_perc\", 332: \"rock_perc\", 333: \"loose_rock_perc\", 334: \"loose_rock_perc\", 335: \"ice_perc\",\n",
    "        411: \"wetlands_perc\", 412: \"wetlands_perc\", 421: \"wetlands_perc\", 422: \"wetlands_perc\", 423: \"wetlands_perc\",\n",
    "        511: \"inwater_perc\", 512: \"inwater_perc\", 521: \"inwater_perc\", 522: \"inwater_perc\", 523: \"inwater_perc\",\n",
    "        999: \"blank_perc\", 990: \"blank_perc\", 995: \"inwater_perc\"\n",
    "    }\n",
    "    # Convert string codes to integers for mapping\n",
    "    clc[\"reclass\"] = clc[code_column].astype(int).map(reclass_dict).fillna(\"na\")\n",
    "    return clc\n",
    "\n",
    "def clip_clc_to_catchments(catchments, clc, code_col=\"reclass\", id_col=\"gauge_id\"):\n",
    "    \"\"\"Clip land cover data to catchments and aggregate area per reclassified class.\"\"\"\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(catchments)), desc=\"Processing catchments\"):\n",
    "        catch_i = catchments.iloc[[i]]\n",
    "        catch_id = catch_i[id_col].values[0]\n",
    "\n",
    "        try:\n",
    "            clc_i = gpd.overlay(clc, catch_i, how=\"intersection\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if clc_i.empty:\n",
    "            continue\n",
    "\n",
    "        clc_i[\"area\"] = clc_i.geometry.area\n",
    "        clc_agg = clc_i.groupby(code_col)[\"area\"].sum().reset_index()\n",
    "        clc_agg.columns = [code_col, catch_id]\n",
    "\n",
    "        if df_all.empty:\n",
    "            df_all = clc_agg\n",
    "        else:\n",
    "            df_all = pd.merge(df_all, clc_agg, on=code_col, how=\"outer\")\n",
    "\n",
    "    return df_all\n",
    "\n",
    "def calculate_percentage_table(area_df, catchments):\n",
    "    \"\"\"Convert area table to percentage based on catchment area.\"\"\"\n",
    "    area_df = area_df.set_index(\"reclass\")\n",
    "    area_df = area_df.fillna(0)\n",
    "    catchment_areas = pd.Series(catchments.geometry.area.values, index=catchments[\"gauge_id\"].values)\n",
    "    percentage_df = area_df.copy()\n",
    "    for col in area_df.columns:\n",
    "        percentage_df[col] = 100 * area_df[col] / catchment_areas[col]\n",
    "    return percentage_df\n",
    "\n",
    "def determine_dominant_class(percentage_df):\n",
    "    \"\"\"Create static attribute table with dominant land cover class per catchment.\"\"\"\n",
    "    dominant_class = percentage_df.idxmax()\n",
    "    static_df = percentage_df.T.copy()\n",
    "    static_df[\"dom_land_cover\"] = dominant_class\n",
    "    return static_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09048a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing ---\n",
    "#Preallocate table for static attributes\n",
    "all_static_tables = {}\n",
    "\n",
    "for year, filename in clc_by_year.items():\n",
    "    print(f\"\\n🌍 Processing year {year}...\")\n",
    "\n",
    "    # Load and reclassify\n",
    "    clc_fp = os.path.join(GIS_dir, f\"{filename}.shp\")\n",
    "    clc = gpd.read_file(clc_fp)\n",
    "    #clc = clc.to_crs(\"EPSG:3035\")\n",
    "    # Automatically detect code column\n",
    "    code_col = next((col for col in clc.columns if col.lower().startswith(\"code_\")), \"Code\")\n",
    "    # Reclassify based on correct column\n",
    "    clc = reclass_clc(clc, code_column=code_col)\n",
    "\n",
    "    # Clip and aggregate\n",
    "    clipped_area_df = clip_clc_to_catchments(catchments_3035, clc)\n",
    "    percent_df = calculate_percentage_table(clipped_area_df, catchments_3035)\n",
    "\n",
    "    # Save percentage table\n",
    "    percent_df.index.name = \"gauge_id\"\n",
    "    percent_df.T.to_csv(f\"clc_{year}_perc.csv\", sep=\";\", float_format=\"%.2f\")\n",
    "\n",
    "    # Static table\n",
    "    static_df = determine_dominant_class(percent_df)\n",
    "    all_static_tables[year] = static_df\n",
    "\n",
    "# Save final static attribute table from 2000\n",
    "final_static = all_static_tables[\"2000\"]\n",
    "final_static.index.name = \"gauge_id\"\n",
    "final_static = final_static.reset_index()\n",
    "\n",
    "# Reorder columns to match R output (if needed)\n",
    "columns_order = ['gauge_id', 'urban_perc', 'loose_rock_perc', 'grass_perc', 'crop_perc',\n",
    "                 'scrub_perc', 'dwood_perc', 'ewood_perc', 'mixed_wood_perc', 'wetlands_perc',\n",
    "                 'rock_perc', 'ice_perc', 'inwater_perc', 'blank_perc', 'dom_land_cover']\n",
    "final_static = final_static[[col for col in columns_order if col in final_static.columns]]\n",
    "\n",
    "# Save\n",
    "final_static.to_csv(\"CAMELS_CH_landcover_attributes.csv\", sep=\";\", float_format=\"%.2f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c792b57",
   "metadata": {},
   "source": [
    "Now we have our 6-yearly landcover for all catchments together with a static landcover, with 2000 as reference year (which is more or less in the middle, if we consider the full range of data spans between 1980 and 2021).\n",
    "\n",
    "Finally, we interpolate linearly between the available years and also generate a file per catchment (similarly to what we did for CAMELS-CH, refer to https://github.com/camels-ch/camels-ch/blob/main/landcover_attributes/annual_timeserie_CH.R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory for the interpolated time series\n",
    "interpolated_dir = os.path.join(PATH_OUTPUT, \"annual_timeseries\")\n",
    "os.makedirs(interpolated_dir, exist_ok=True)\n",
    "\n",
    "# Automatically detect available years based on filenames\n",
    "clc_files = [f for f in os.listdir(PATH_OUTPUT) if f.startswith(\"clc_\") and f.endswith(\"_perc.csv\")]\n",
    "clc_years = sorted([int(f.split(\"_\")[1]) for f in clc_files])\n",
    "\n",
    "# Load all available CLC data and store by year\n",
    "clc_data_by_year = {}\n",
    "for year in clc_years:\n",
    "    df = pd.read_csv(f\"clc_{year}_perc.csv\", sep=\";\", index_col=0)\n",
    "    clc_data_by_year[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c37cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a full range of years from the available years\n",
    "full_years = list(range(clc_years[0], clc_years[-1] + 1))       \n",
    "\n",
    "# Get all catchments and land cover classes\n",
    "all_catchments = clc_data_by_year[2000].index.tolist()\n",
    "landcover_classes = clc_data_by_year[2000].columns.tolist()\n",
    "\n",
    "# Preallocate final dataframe: MultiIndex with (gauge_id, year)\n",
    "index = pd.MultiIndex.from_product([all_catchments, full_years], names=[\"gauge_id\", \"year\"])\n",
    "# Uncomment the following line if you don't want to generate a file with annual timeseries for all catchments)\n",
    "landcover_timeseries = pd.DataFrame(index=index, columns=landcover_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate for each catchment\n",
    "for gauge in tqdm.tqdm(all_catchments, desc=\"Interpolating time series\"):\n",
    "    catchment_ts = pd.DataFrame(index=clc_years, columns=landcover_classes, dtype=float)\n",
    "    catchment_ts.index = catchment_ts.index.astype(int)\n",
    "    catchment_ts = catchment_ts.reindex(full_years)\n",
    "\n",
    "    for year in clc_years:\n",
    "        catchment_ts.loc[year] = clc_data_by_year[year].loc[gauge].astype(float)\n",
    "\n",
    "    # Interpolate\n",
    "    catchment_ts_interp = catchment_ts.astype(float).interpolate(method=\"linear\", axis=0).reindex(full_years)\n",
    "\n",
    "    # Align columns before assignment\n",
    "    catchment_ts_interp = catchment_ts_interp[catchment_ts.columns]\n",
    "\n",
    "    # Store (uncomment the following two lines if you don't want to generate a file with annual timeseries for all catchments)\n",
    "    for year in full_years:\n",
    "        landcover_timeseries.loc[(gauge, year), :] = catchment_ts_interp.loc[year]\n",
    "    \n",
    "    # Save individual CSV for each catchment\n",
    "    catchment_ts_interp.index.name = \"year\"\n",
    "    catchment_ts_interp.to_csv(os.path.join(interpolated_dir, f\"CAMELS_CH_Chem_landcover_{gauge}_annual_timeseries.csv\"), sep=\";\", float_format=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ecce5",
   "metadata": {},
   "source": [
    "Run the next lines of code if you want to generate a file with annual timeseries for all catchments, otherwise yu are done, yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index and save to file\n",
    "landcover_timeseries = landcover_timeseries.reset_index()\n",
    "output_filename = f\"CAMELS_CH_Chem_landcover_annual_timeseries_{clc_years[0]}_{clc_years[-1]}.csv\"\n",
    "landcover_timeseries.to_csv(output_filename, sep=\";\", float_format=\"%.2f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad2fa0",
   "metadata": {},
   "source": [
    "Adjust the name of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_2020 = \"../results/landcover/annual_timeseries\"\n",
    "output_folder = \"../results/Dataset/catchment_aggregated_data/landcover_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(folder_2020):\n",
    "    if filename.startswith(\"CAMELS_CH_Chem_landcover_\") and filename.endswith(\"_annual_timeseries.csv\"):\n",
    "        \n",
    "        # Extract basin code\n",
    "        parts = filename.split(\"_\")\n",
    "        basin_code = parts[4]\n",
    "\n",
    "        # Load file\n",
    "        input_path = os.path.join(folder_2020, filename)\n",
    "        df = pd.read_csv(input_path, sep=\";\")\n",
    "        df.columns = ['date', 'crop_perc', 'dwood_perc', 'ewood_perc', 'grass_perc',\n",
    "       'ice_perc', 'inwater_perc', 'loose_rock_perc', 'mixed_wood_perc',\n",
    "       'rock_perc', 'scrub_perc', 'urban_perc', 'wetlands_perc'] \n",
    "        # Repeat the last row (assumed to be 2018) for 2019 and 2020\n",
    "        if not df.empty:\n",
    "            last_row = df.iloc[-1].copy()\n",
    "\n",
    "            for year in [2019, 2020]:\n",
    "                new_row = last_row.copy()\n",
    "                new_row[df.columns[0]] = year  # Assumes first column is 'year'\n",
    "                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        # Create new filename\n",
    "        new_filename = f\"camels_ch_chem_landcover_{basin_code}.csv\"\n",
    "        output_path = os.path.join(output_folder, new_filename)\n",
    "\n",
    "        # Save\n",
    "        df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608e26e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

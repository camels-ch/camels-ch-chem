{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Sensors hourly dataset extraction\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is used to retrieve and concatenate the sensors dataset into hourly resolutions. The output is one file per catchemnt (similar to the CAMELS-CH), with 5 columns: date, ec_sensor(µS/cm), pH_sensor(-), temp_sensor(°C) & O2C_sensor(mg/l)\n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* warnings\n",
    "* numpy\n",
    "* os\n",
    "* pandas=2.1.3\n",
    "* glob\n",
    "* tqdm\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* All the physical data provided from BAFU (\"phys_daten_BAFU\")\n",
    "\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "\n",
    "## References\n",
    "* BAFU. Federal Office for the Environment, Switzerland. https://www.bafu.admin.ch/bafu/en/home.html (last access: 20 Sep 2024).\n",
    "## Observations\n",
    "* None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a0050",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Relative path to your local directory\n",
    "PATH = \"../..\"\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Path to where the sensors data are stored\n",
    "path_sensors = r\"C:\\Users\\nascimth\\Documents\\data\\CAMELS_CH_Chem\\data\\phys_daten_BAFU\\phys_daten_BAFU\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c1c1e",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_OUTPUT = r\"results\\Dataset\\stream_water_chemistry\\timeseries\"\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54907f",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "filenames = glob.glob(path_sensors + \"*.csv\")\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b671e5",
   "metadata": {},
   "source": [
    "* Electric conductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6518bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two specific words you want to filter for\n",
    "word1 = 'Elektrische'\n",
    "word2 = 'Stundenmittel'\n",
    "\n",
    "# Filter CSV files based on whether their names contain both words\n",
    "electric_filenames = [file for file in filenames if (word1 in file) and (word2 in file)]\n",
    "len(electric_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b70904",
   "metadata": {},
   "source": [
    "* BAFU: 2290 is located elsewhere (2290_St-Sulpice_Areuse_Eawag_CAMELS_CH)\n",
    "* NADUF: 2046, 2044 and 2045 (not BAFU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fcb17",
   "metadata": {},
   "source": [
    "* pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two specific words you want to filter for\n",
    "word1 = 'pH'\n",
    "word2 = 'Stundenmittel'\n",
    "\n",
    "# Filter CSV files based on whether their names contain both words\n",
    "ph_filenames = [file for file in filenames if (word1 in file) and (word2 in file)]\n",
    "ph_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051586c",
   "metadata": {},
   "source": [
    "Here we have only 2622 (2243). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d1c32",
   "metadata": {},
   "source": [
    "* Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99881ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two specific words you want to filter for\n",
    "word1 = 'Wassertemperatur'\n",
    "word2 = 'Stundenmittel'\n",
    "\n",
    "# Filter CSV files based on whether their names contain both words\n",
    "temperature_filenames = [file for file in filenames if (word1 in file) and (word2 in file)]\n",
    "temperature_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efd7ef",
   "metadata": {},
   "source": [
    "* BAFU: 2290 is located elsewhere (2290_St-Sulpice_Areuse_Eawag_CAMELS_CH), 2176 (Kanton ZH) is located elsewhere.\n",
    "\n",
    "* NADUF: 2046, 2044, 2045 (not BAFU).\n",
    "\n",
    "* 2622 (2243) is included in both files here for temperature, but suposelly is the same (ask Ursi again). \n",
    "\n",
    "* 2190, 2424 and 2425 are included here, but not in CAMELS or in the original list from Ursi (88 files in Hydronetz), but are in the excel and files. \n",
    "\n",
    "* This give us a total of 90 stations with temperature, which is the same number of total stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199df568",
   "metadata": {},
   "source": [
    "* Oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two specific words you want to filter for\n",
    "word1 = 'Sauerstoff'\n",
    "word2 = 'Stundenmittel'\n",
    "\n",
    "# Filter CSV files based on whether their names contain both words\n",
    "oxygen_filenames = [file for file in filenames if (word1 in file) and (word2 in file)]\n",
    "oxygen_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa1f79",
   "metadata": {},
   "source": [
    "Here we have only 2622 (2243). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e70c7d",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a78bb7",
   "metadata": {},
   "source": [
    "* We have 90 stations in total.\n",
    "* The 88 from the temperature list + 2290 + 2176.\n",
    "* We first aggregate the data available from BAFU, then the 2 extra stations.\n",
    "\n",
    "#### - BAFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1402c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = [\"temp_sensor\", \"pH_sensor\", \"ec_sensor\", \"O2C_sensor\"]\n",
    "\n",
    "for filename in tqdm.tqdm(temperature_filenames):\n",
    "\n",
    "    hourly_timeseries = pd.DataFrame(index = pd.date_range('01-01-1981','12-31-2020', freq='H'), columns=columns_names)\n",
    "    \n",
    "    network_filename = pd.read_csv(filename, skiprows=8, delimiter = \";\", encoding='latin-1', usecols=[\"Zeitstempel\", \n",
    "                                                                                                      \"Wert\"])\n",
    "    network_filename[\"date\"] = pd.to_datetime(network_filename[\"Zeitstempel\"], format='%Y-%m-%d %H:%M:%S')\n",
    "    network_filename = network_filename[[\"date\", \"Wert\"]]\n",
    "    network_filename.set_index(\"date\", inplace = True)\n",
    "    \n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 5)[0]\n",
    "    hourly_timeseries[\"temp_sensor\"] = network_filename[\"Wert\"]\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    ##  pH\n",
    "    \n",
    "    filenames = ph_filenames.copy()\n",
    "    # List to store filenames containing the number\n",
    "    matching_filenames = []\n",
    "\n",
    "    # Iterate over filenames and check if the number is present in the first 4 characters of the file name\n",
    "    for filename in filenames:\n",
    "        # Extract the file name (without the path)\n",
    "        file_name_only = filename.split(\"\\\\\")[-1]\n",
    "        # Extract the first 4 characters of the file name\n",
    "        first_four_characters = file_name_only[:4]\n",
    "        # Check if the number is present in the first 4 characters\n",
    "        if namestation in first_four_characters:\n",
    "            matching_filenames.append(filename)\n",
    "    try:        \n",
    "        network_filename = pd.read_csv(matching_filenames[0], skiprows=8, delimiter = \";\", encoding='latin-1', usecols=[\"Zeitstempel\", \n",
    "                                                                                                          \"Wert\"])\n",
    "        network_filename[\"date\"] = pd.to_datetime(network_filename[\"Zeitstempel\"], format='%Y-%m-%d %H:%M:%S')\n",
    "        network_filename = network_filename[[\"date\", \"Wert\"]]\n",
    "        network_filename.set_index(\"date\", inplace = True)\n",
    "        network_filename['Wert'] = pd.to_numeric(network_filename['Wert'], errors='coerce')\n",
    "\n",
    "        hourly_timeseries[\"pH_sensor\"] = network_filename[\"Wert\"]      \n",
    "    except: \n",
    "        1+1\n",
    "        \n",
    "    ###################################################\n",
    "    ##  electric_cond(µS/cm)\n",
    "    \n",
    "    filenames = electric_filenames.copy()\n",
    "    # List to store filenames containing the number\n",
    "    matching_filenames = []\n",
    "\n",
    "    # Iterate over filenames and check if the number is present in the first 4 characters of the file name\n",
    "    for filename in filenames:\n",
    "        # Extract the file name (without the path)\n",
    "        file_name_only = filename.split(\"\\\\\")[-1]\n",
    "        # Extract the first 4 characters of the file name\n",
    "        first_four_characters = file_name_only[:4]\n",
    "        # Check if the number is present in the first 4 characters\n",
    "        if namestation in first_four_characters:\n",
    "            matching_filenames.append(filename)\n",
    "    try:       \n",
    "        network_filename = pd.read_csv(matching_filenames[0], skiprows=8, delimiter = \";\", encoding='latin-1', usecols=[\"Zeitstempel\", \n",
    "                                                                                                          \"Wert\"])\n",
    "        network_filename[\"date\"] = pd.to_datetime(network_filename[\"Zeitstempel\"], format='%Y-%m-%d %H:%M:%S')\n",
    "        network_filename = network_filename[[\"date\", \"Wert\"]]\n",
    "        network_filename.set_index(\"date\", inplace = True)\n",
    "        network_filename['Wert'] = pd.to_numeric(network_filename['Wert'], errors='coerce')\n",
    "\n",
    "        hourly_timeseries[\"ec_sensor\"] = network_filename[\"Wert\"]\n",
    "    except:\n",
    "        1+1\n",
    "    ###################################################\n",
    "    ##  oxygen_conc(mg/l)\n",
    "    \n",
    "    filenames = oxygen_filenames.copy()\n",
    "    # List to store filenames containing the number\n",
    "    matching_filenames = []\n",
    "\n",
    "    # Iterate over filenames and check if the number is present in the first 4 characters of the file name\n",
    "    for filename in filenames:\n",
    "        # Extract the file name (without the path)\n",
    "        file_name_only = filename.split(\"\\\\\")[-1]\n",
    "        # Extract the first 4 characters of the file name\n",
    "        first_four_characters = file_name_only[:4]\n",
    "        # Check if the number is present in the first 4 characters\n",
    "        if namestation in first_four_characters:\n",
    "            matching_filenames.append(filename)\n",
    "    try:      \n",
    "        network_filename = pd.read_csv(matching_filenames[0], skiprows=8, delimiter = \";\", encoding='latin-1', usecols=[\"Zeitstempel\", \n",
    "                                                                                                          \"Wert\"])\n",
    "        network_filename[\"date\"] = pd.to_datetime(network_filename[\"Zeitstempel\"], format='%Y-%m-%d %H:%M:%S')\n",
    "        network_filename = network_filename[[\"date\", \"Wert\"]]\n",
    "        network_filename.set_index(\"date\", inplace = True)\n",
    "        network_filename['Wert'] = pd.to_numeric(network_filename['Wert'], errors='coerce')\n",
    "\n",
    "        hourly_timeseries[\"O2C_sensor\"] = network_filename[\"Wert\"]  \n",
    "    except: \n",
    "        1+1\n",
    "    \n",
    "    hourly_timeseries.index.name = \"date\"\n",
    "    \n",
    "    hourly_timeseries = hourly_timeseries.round(2)\n",
    "\n",
    "    hourly_timeseries.to_csv(PATH_OUTPUT + \"/hourly/camels_ch_chem_hourly_\"+namestation+\".csv\", encoding='latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179d61d",
   "metadata": {},
   "source": [
    "#### - 2 extra stations\n",
    "Station 2176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "namestation = \"2176\"\n",
    "\n",
    "columns_names = [\"temperature(°C)\", \"pH(-)\", \"electric_cond(µS/cm)\", \"oxygen_conc(mg/l)\"]\n",
    "hourly_timeseries = pd.DataFrame(index = pd.date_range('01-01-1981','12-31-2020', freq='H'), columns=columns_names)\n",
    "\n",
    "path_2176 = r'C:\\\\Users\\\\nascimth\\\\Documents\\\\data\\\\CAMELS_CH_Chem\\\\data\\\\Kanton ZH\\S2176\\S2176_Wassertemperatur _Stundenmittel.txt'\n",
    "network_2176 = pd.read_csv(path_2176, skiprows=2, delimiter = \"\\t\", encoding='latin-1')\n",
    "network_2176[\"date\"] = network_2176[\"Datum\"] + \" \"  + network_2176[\"Zeit\"]\n",
    "network_2176[\"date\"] = pd.to_datetime(network_2176[\"date\"], format='%d.%m.%Y %H:%M:%S')\n",
    "network_2176.set_index(\"date\", inplace = True)\n",
    "network_2176 = network_2176[[\"Messwert\"]]\n",
    "hourly_timeseries[\"temperature(°C)\"] = network_2176[\"Messwert\"]\n",
    "\n",
    "hourly_timeseries.index.name = \"date\"\n",
    "    \n",
    "hourly_timeseries = hourly_timeseries.round(2)\n",
    "\n",
    "# Rename the names of the columns\n",
    "hourly_timeseries.columns = [\"temp_sensor\", \"pH_sensor\", \"ec_sensor\", \"O2C_sensor\"]\n",
    "\n",
    "hourly_timeseries.to_csv(PATH_OUTPUT + \"/hourly/camels_ch_chem_hourly_\"+namestation+\".csv\", encoding='latin')\n",
    "\n",
    "hourly_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b15dfe",
   "metadata": {},
   "source": [
    "Station 2290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "namestation = \"2290\"\n",
    "\n",
    "columns_names = [\"temperature(°C)\", \"pH(-)\", \"electric_cond(µS/cm)\", \"oxygen_conc(mg/l)\"]\n",
    "hourly_timeseries = pd.DataFrame(index = pd.date_range('01-01-1981','12-31-2020', freq='H'), columns=columns_names)\n",
    "\n",
    "############ Electrical conductivity\n",
    "path_2290_EC = r'C:\\\\Users\\\\nascimth\\\\Documents\\\\data\\\\CAMELS_CH_Chem\\\\data\\\\S2290\\S2290_St-Sulpice-Areuse_LF_20090425-20221231.CSV'\n",
    "\n",
    "network_2290_EC = pd.read_csv(path_2290_EC, skiprows=13, delimiter = \";\", encoding='latin-1')\n",
    "\n",
    "network_2290_EC = network_2290_EC[[\"Datum.1\", \"Uhrzeit.1\", \"LF Stundenmittel [µS/cm]\"]]\n",
    "\n",
    "# Drop rows with NaN values in the specific column\n",
    "network_2290_EC.dropna(subset=['Datum.1'], inplace=True)\n",
    "\n",
    "network_2290_EC[\"date\"] = network_2290_EC[\"Datum.1\"] + \" \"  + network_2290_EC[\"Uhrzeit.1\"]\n",
    "network_2290_EC[\"date\"] = pd.to_datetime(network_2290_EC[\"date\"], format='%d.%m.%Y %H:%M:%S')\n",
    "network_2290_EC.set_index(\"date\", inplace = True)\n",
    "\n",
    "############ Temperature\n",
    "path_2290_T = r'C:\\\\Users\\\\nascimth\\\\Documents\\\\data\\\\CAMELS_CH_Chem\\\\data\\\\S2290\\S2290_St-Sulpice-Areuse_T_20090425-20221231.CSV'\n",
    "\n",
    "network_2290_T = pd.read_csv(path_2290_T, skiprows=13, delimiter = \";\", encoding='latin-1')\n",
    "\n",
    "network_2290_T = network_2290_T[[\"Datum.1\", \"Uhrzeit.1\", \"T Stundenmittel [°C]\"]]\n",
    "\n",
    "# Drop rows with NaN values in the specific column\n",
    "network_2290_T.dropna(subset=['Datum.1'], inplace=True)\n",
    "\n",
    "network_2290_T[\"date\"] = network_2290_T[\"Datum.1\"] + \" \"  + network_2290_T[\"Uhrzeit.1\"]\n",
    "network_2290_T[\"date\"] = pd.to_datetime(network_2290_T[\"date\"], format='%d.%m.%Y %H:%M:%S')\n",
    "network_2290_T.set_index(\"date\", inplace = True)\n",
    "\n",
    "############################################################################\n",
    "network_2290_EC = network_2290_EC[[\"LF Stundenmittel [µS/cm]\"]]\n",
    "hourly_timeseries[\"electric_cond(µS/cm)\"] = network_2290_EC[\"LF Stundenmittel [µS/cm]\"]\n",
    "\n",
    "network_2290_T = network_2290_T[[\"T Stundenmittel [°C]\"]]\n",
    "hourly_timeseries[\"temperature(°C)\"] = network_2290_T[\"T Stundenmittel [°C]\"]\n",
    "\n",
    "hourly_timeseries.index.name = \"date\"\n",
    "\n",
    "# There are some non-numeric things in the columns, instead of NaNs\n",
    "hourly_timeseries = hourly_timeseries.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "hourly_timeseries = hourly_timeseries.round(2)\n",
    "\n",
    "# Rename the names of the columns\n",
    "hourly_timeseries.columns = [\"temp_sensor\", \"pH_sensor\", \"ec_sensor\", \"O2C_sensor\"]\n",
    "\n",
    "hourly_timeseries.to_csv(PATH_OUTPUT + \"/hourly/camels_ch_chem_hourly_\"+namestation+\".csv\", encoding='latin')\n",
    "\n",
    "hourly_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77a783",
   "metadata": {},
   "source": [
    "- Remarks\n",
    "\n",
    "    - 87 stations from BAFU in inverntarlist \n",
    "\n",
    "    - 2190, 2424 and 2425 are included here, but not in the internarlist/CAMELS-CH (moved to another folder)\n",
    "\n",
    "    - 2403, (empty).\n",
    "    \n",
    "    - We need to merge 2622 into 2243."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0119cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 2622 into 2243\n",
    "bafu_2622_hourly = pd.read_csv(r\"results\\\\Dataset\\stream_water_chemistry\\\\timeseries\\hourly\\camels_ch_chem_hourly_2622.csv\", encoding='latin')\n",
    "bafu_2622_hourly.set_index(\"date\", inplace=True)\n",
    "bafu_2622_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b28746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 2622 into 2243\n",
    "\n",
    "bafu_2243_hourly = pd.read_csv(r\"results\\Dataset\\stream_water_chemistry\\timeseries\\hourly\\camels_ch_chem_hourly_2243.csv\", encoding='latin')\n",
    "bafu_2243_hourly.set_index(\"date\", inplace=True)\n",
    "bafu_2243_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f928e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bafu_2243_hourly[\"temp_sensor\"].plot()\n",
    "bafu_2622_hourly[\"temp_sensor\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b902e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bafu_2243_hourly[\"ec_sensor\"] = bafu_2622_hourly[\"ec_sensor\"]\n",
    "bafu_2243_hourly[\"pH_sensor\"] = bafu_2622_hourly[\"pH_sensor\"]\n",
    "bafu_2243_hourly[\"O2C_sensor\"] = bafu_2622_hourly[\"O2C_sensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5945827",
   "metadata": {},
   "outputs": [],
   "source": [
    "bafu_2243_hourly.to_csv(r\"results\\Dataset\\stream_water_chemistry\\timeseries\\hourly\\camels_ch_chem_hourly_2243.csv\", encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we delete the not needed files: \n",
    "# Get all CSV files in the current directory\n",
    "csv_files = [\"results\\\\Dataset\\stream_water_chemistry\\\\timeseries\\hourly\\camels_ch_chem_hourly_2622.csv\",\n",
    "             \"results\\\\Dataset\\stream_water_chemistry\\\\timeseries\\hourly\\camels_ch_chem_hourly_2190.csv\", \n",
    "             \"results\\\\Dataset\\stream_water_chemistry\\\\timeseries\\hourly\\camels_ch_chem_hourly_2424.csv\",\n",
    "             \"results\\\\Dataset\\stream_water_chemistry\\\\timeseries\\hourly\\camels_ch_chem_hourly_2425.csv\"]\n",
    "\n",
    "# Delete each file\n",
    "for file in csv_files:\n",
    "    os.remove(file)\n",
    "    print(f\"Deleted: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608e26e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
